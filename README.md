## Repository Description
In this repository, there are three (3) folders that contain the codes strings and files that are used for the pre-processing and processing stages of the project. These folders are named 'input', 'output', and 'sql'. The 'input' and 'sql' folders are used for harbouring the code that is used for the pre-processing stage of the project. Seeming straigtforward, the 'sql' folder contains the SQL code strings used to create and join the tables required for processing data in pgAdmin.  Within the 'input' folder, there consists a Jupyter notebook that contains the code used for accessing tables from pgAdmin and reading stored CSV files. The Python codes this notebook contains is based from the SQL code that was initially written. Each input also has a code in string form that explains what is being conducted at that particular stage.

As for the 'output' folder, there contains a separate Jupyter notebook that contains the code used to conduct random forest analysis and geo-regression on the dataframe. With each input, similar to the notebook used for pre-processing, there is also a string line that explains what is being conducted at that particular stage. Within this notebook, the visualisations used for this project are also presented. Finally, aside from the folders and documents containing the codes used for this project, the project repository also contains default GitHub instructions and this README file.

## Data Pre-processing
Jupyter notebook named __*pre-processing.ipynb*__ executes the following workflow of pre-processing data to generate observer intensity for blocks within Netherlands for the time period *2017-01-01* and *2017-06-30*, along with other attributes.
1. **Read** csv files of weather (*weather.csv*), observer intensity (*ObsIntTable.csv*) and road length (*roadlen.csv*).
2. **Read** sql table of blocks and demography as GeoDataFrame 
3. **Spatial join** blocks and demography. Blocks with no population are set to 0.
4. **Convert** observation and weather dates into seconds with 0 as *2017-01-01* in observer intensity and weather dataframes.
5. **Merge** (right join) observer intensity and weather on block number and observation time. When there are no observations, observer intensity is set as 0.
6. **Merge** (left join) demography with road length on block number. Blocks with null road length are set as 0.
7. **Merge** (left join) 5 and 6 on block number.
8. **Select** attributes: *block*,*longit*,*latit*,*obstime*,*temper*,*precip*,*pop*,*roadlen*,*obsint* and export the dataframe to csv file: *obsInt_obsTime_with_variables.csv*. This file will be created in __*output*__ folder for analysis.\
**Note**: *weather.csv* files is around 216MB and cannot be uploaded to GitHub. You can to access the [Google drive](https://drive.google.com/file/d/1PexJKTYSHFLWrQWdGWZZ_zfKw2Ka8OqA/view?usp=sharing) file with University of Twente email. Download the file and store in the __*input*__ folder

## Data Processing and Output
For this project, the group decided to conduct random forest analysis and geo-regression on the processed dataset. This was done to find a model that can predict observer intensity. To conduct this analysis, the dataframe was first transformed into an array, and was split into training and test arrays. At this stage, the x-values and y-values were also determined based on the processed dataframe from SQL to Python. This step was required for conducting a prediction analysis. Afterwards, the code for random forest was implemented on the training and test arrays in their respective places. The output from this stage was the R-squared value of the model as well as the importance feature for each predictor on observer intensity. From this random forest output, we then visualise the geo-regression model onto a chart and a map of the Netherlands. The visualisations show a compare the predicted and real y-values, and show the overall observer intensity throughout the given time period respectively. The overall steps for this stage is also given in the project report.
